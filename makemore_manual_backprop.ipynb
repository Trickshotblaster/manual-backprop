{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of names: 32033\n",
      "Max name length: 15\n",
      "First 8 names: ['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\n"
     ]
    }
   ],
   "source": [
    "with open(\"names.txt\", \"r\") as f:\n",
    "    words = f.read().splitlines()\n",
    "print(f\"Number of names: {len(words)}\")\n",
    "print(f\"Max name length: {max(len(word) for word in words)}\")\n",
    "print(f\"First 8 names: {words[:8]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 27\n",
      "{'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26, '.': 0}\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(set(\"\".join(words)))\n",
    "stoi = {char:i+1 for i, char in enumerate(chars)}\n",
    "stoi[\".\"] = 0\n",
    "itos = {i:char for i, char in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "print(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([131359, 3]) torch.Size([131359, 1])\n",
      "torch.Size([16416, 3]) torch.Size([16416, 1])\n",
      "torch.Size([16305, 3]) torch.Size([16305, 1])\n"
     ]
    }
   ],
   "source": [
    "block_size = 3\n",
    "\n",
    "def build_dataset(words):\n",
    "    x, y = [], []\n",
    "    for word in words:\n",
    "        context = [0] * block_size\n",
    "        for char, next in zip(word, word[1:]):\n",
    "            x.append(context)\n",
    "            y.append([stoi[next]])\n",
    "            context = context[1:] + [stoi[char]]\n",
    "    x = torch.tensor(x)\n",
    "    y = torch.tensor(y)\n",
    "    print(x.shape, y.shape)\n",
    "    return x, y\n",
    "\n",
    "import random\n",
    "random.shuffle(words)\n",
    "split1 = int(0.8*len(words))\n",
    "split2 = int(0.9*len(words))\n",
    "\n",
    "Xtr, Ytr = build_dataset(words[:split1])\n",
    "Xdev, Ydev = build_dataset(words[split1:split2])\n",
    "Xte, Yte = build_dataset(words[split2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cmp(s, dt, t):\n",
    "    ex = torch.all(dt == t.grad).item()\n",
    "    app = torch.allclose(dt, t.grad)\n",
    "    maxdiff = (dt - t.grad).abs().max().item()\n",
    "    print(f\"{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4137\n"
     ]
    }
   ],
   "source": [
    "n_embd = 10\n",
    "n_hidden = 64\n",
    "\n",
    "C = torch.randn((vocab_size, n_embd))\n",
    "w1 = torch.randn((n_embd * block_size, n_hidden)) * (5/3)/((n_embd*block_size)**0.5)\n",
    "b1 = torch.randn((n_hidden)) * 0.1\n",
    "w2 = torch.randn((n_hidden, vocab_size)) *0.1#/ (n_embd * block_size)**0.5\n",
    "b2 = torch.randn((vocab_size)) * 0.1\n",
    "\n",
    "bngain = torch.randn((1, n_hidden)) * 0.1 + 1.0\n",
    "bnbias = torch.randn((1, n_hidden)) * 0.1\n",
    "\n",
    "parameters = [C, w1, b1, w2, b2, bngain, bnbias]\n",
    "print(sum(p.numel() for p in parameters))\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "n = batch_size\n",
    "\n",
    "idx = torch.randint(0, Xtr.shape[0], (batch_size,))\n",
    "Xb, Yb = Xtr[idx], Ytr[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.3435, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embedding\n",
    "emb = C[Xb]\n",
    "embcat = emb.view(batch_size, -1)\n",
    "# layer 1\n",
    "hprebn = embcat @ w1 + b1\n",
    "# batchnorm\n",
    "bnmeani = hprebn.sum(0, keepdim=True) * (1/n) # = sum/n = mean\n",
    "bndiff = hprebn - bnmeani\n",
    "bndiff2 = bndiff ** 2\n",
    "bnvar = bndiff2.sum(0, keepdim=True) * (1/(n-1))\n",
    "bnvar_inv = (bnvar + 1e-5) ** -0.5\n",
    "bnraw = bndiff * bnvar_inv # (x-mean) / std\n",
    "hpreact = bngain * bnraw + bnbias\n",
    "# output\n",
    "h = torch.tanh(hpreact)\n",
    "logits = h @ w2 + b2\n",
    "\n",
    "# loss (cross-entropy)\n",
    "logit_maxes = logits.max(1, keepdim=True).values\n",
    "norm_logits = logits - logit_maxes\n",
    "counts = norm_logits.exp()\n",
    "counts_sum = counts.sum(1, keepdim=True)\n",
    "counts_sum_inv = counts_sum ** -1\n",
    "probs = counts * counts_sum_inv\n",
    "logprobs = probs.log()\n",
    "loss = -logprobs[range(n), Yb].mean()\n",
    "\n",
    "# pytorch backward\n",
    "for p in parameters:\n",
    "    p.grad = None\n",
    "for t in [logprobs, probs, counts, counts_sum, counts_sum_inv,\n",
    "           norm_logits, logit_maxes, logits, h, hpreact,\n",
    "             bnraw, bnvar_inv, bnvar, bndiff2, bndiff, hprebn,\n",
    "               bnmeani, embcat, emb]:\n",
    "    t.retain_grad()\n",
    "loss.backward()\n",
    "loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for p in parameters:\n",
    "    plt.hist(p.grad.detach().view(-1).tolist())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3.7637e-02, -1.7174e-01,  3.0518e-02,  4.5592e-03,  3.8816e-02,\n",
      "          4.1677e-03,  3.1791e-02,  4.2389e-02,  6.8519e-03, -5.8619e-02,\n",
      "          3.6213e-02,  3.8468e-02, -1.4892e-01, -2.2852e-02,  2.8249e-02,\n",
      "          3.9224e-02,  4.0476e-02,  3.7256e-02, -1.5436e-01,  7.5215e-05,\n",
      "          8.6526e-03,  2.6823e-02,  3.6337e-02,  4.4611e-02,  3.5114e-02,\n",
      "         -4.9955e-02,  3.8218e-02]], grad_fn=<MulBackward0>)\n",
      "tensor([ 3.7637e-02, -1.7174e-01,  3.0518e-02,  4.5592e-03,  3.8816e-02,\n",
      "         4.1677e-03,  3.1791e-02,  4.2389e-02,  6.8519e-03, -5.8619e-02,\n",
      "         3.6213e-02,  3.8468e-02, -1.4892e-01, -2.2852e-02,  2.8249e-02,\n",
      "         3.9224e-02,  4.0476e-02,  3.7256e-02, -1.5436e-01,  7.5215e-05,\n",
      "         8.6526e-03,  2.6823e-02,  3.6337e-02,  4.4611e-02,  3.5114e-02,\n",
      "        -4.9955e-02,  3.8218e-02])\n",
      "logprobs        | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "probs           | exact: False | approximate: True  | maxdiff: 2.9802322387695312e-08\n",
      "counts_sum_inv  | exact: False | approximate: True  | maxdiff: 2.9802322387695312e-08\n",
      "counts_sum      | exact: False | approximate: True  | maxdiff: 4.656612873077393e-10\n",
      "counts          | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n",
      "norm_logits     | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n",
      "logit_maxes     | exact: False | approximate: True  | maxdiff: 7.916241884231567e-09\n",
      "logits          | exact: False | approximate: True  | maxdiff: 7.916241884231567e-09\n",
      "h               | exact: False | approximate: True  | maxdiff: 2.0954757928848267e-09\n",
      "w2              | exact: False | approximate: False | maxdiff: 6.332993507385254e-08\n",
      "b2              | exact: False | approximate: True  | maxdiff: 7.450580596923828e-08\n"
     ]
    }
   ],
   "source": [
    "# Exercise 1: backprop through the whole thing manually,\n",
    "# backpropagating through exactly all of the variables\n",
    "# as they are defined in the forward pass above, one by one\n",
    "\n",
    "dlogprobs = torch.zeros_like(logprobs) \n",
    "for b in range(n):\n",
    "    for y in Yb:\n",
    "        dlogprobs[b, y] += -1/logprobs[range(n), Yb].numel()\n",
    "\n",
    "dprobs = dlogprobs * (1/probs) # chain rule\n",
    "dcounts_sum_inv = (dprobs * counts).sum(1, keepdim=True) # I guess we have to apply all shape-changing operations to derivative\n",
    "dcounts_sum = dcounts_sum_inv * -counts_sum**-2\n",
    "# dloss/dcounts = dloss/dcounts_sum * dcounts_sum/dcounts\n",
    "# dcounts_sum/d_counts should be 1s in the shape of counts\n",
    "# assuming counts is [[a, b], [c, d]],\n",
    "# counts_sum = \n",
    "# [[a, b],\n",
    "# [c, d]].sum(1, keepdim=True)\n",
    "# --> \n",
    "# [[a+c],\n",
    "# [b+d]]\n",
    "# Therefore dcounts_sum/da = 1 + 0 = 1\n",
    "# dcounts_sum/db = 0 + 1 = 1\n",
    "# etc ->\n",
    "# counts_grad = \n",
    "# [[1, 1],\n",
    "# [1, 1]]\n",
    "# So, by chain rule, dloss/dcounts = dloss/dcounts_sum * dcounts_sum/dcounts\n",
    "# = dloss/dcounts_sum * counts_grad (ones in shape of counts)\n",
    "# counts are also used in calculation of probs\n",
    "# so just add dcounts/dloss using chain rule with probs\n",
    "dcounts = dcounts_sum * torch.ones_like(counts) + counts_sum_inv * dprobs\n",
    "dnorm_logits = counts * dcounts\n",
    "# ok so max is a little weird\n",
    "# imagine you have a tensor a = [[0.2, 6.9], [-1.4, 3.2]]\n",
    "# and then you take the max of it and transform that to get an output\n",
    "# if you shift a by a tiny amount to get the gradient of the max of a,\n",
    "# the max of a will not change, therefore the gradient of the max is zero\n",
    "dlogit_maxes = -1 * torch.zeros_like(logit_maxes)\n",
    "dlogits = 0 + dnorm_logits * 1 # dloss/dlogit_maxes * dlogit_maxes/dlogits + dloss/dnorm_logits * dnorm_logits/dlogits\n",
    "dh = dlogits @ w2.T # dloss/dlogits * dlogits/dh = dlogits * w2 (derivative of matmul is to matmul by transpose??)\n",
    "dw2 = h.T @ dlogits # ok so its matmul by transpose but make sure its on the same side as before\n",
    "# also it no longer says its an approximate here but the max diff is 6e-8 which seems like floating point error to me\n",
    "db2 = torch.ones_like(b2) * dlogits.sum(0, keepdim=True) # dlogits/db2 is just going to be 1s in the shape of b2\n",
    "# but we also need to apply chain rule by multiplying by dloss/dlogits\n",
    "# the problem is that dlogits is (32, 27), while db2 is (27)\n",
    "# we can force dlogits to be (1, 27) by summing along dim 0\n",
    "# tbh idk why this works \\_(\"/)_/\n",
    "\n",
    "print(db2)\n",
    "print(b2.grad)\n",
    "cmp(\"logprobs\", dlogprobs, logprobs)\n",
    "cmp(\"probs\", dprobs, probs)\n",
    "cmp(\"counts_sum_inv\", dcounts_sum_inv, counts_sum_inv)\n",
    "cmp(\"counts_sum\", dcounts_sum, counts_sum)\n",
    "cmp(\"counts\", dcounts, counts)\n",
    "cmp(\"norm_logits\", dnorm_logits, norm_logits)\n",
    "cmp(\"logit_maxes\", dlogit_maxes, logit_maxes)\n",
    "cmp(\"logits\", dlogits, logits)\n",
    "cmp(\"h\", dh, h)\n",
    "cmp(\"w2\", dw2, w2)\n",
    "cmp(\"b2\", db2, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-6.7510e-04,  1.2098e-02, -3.9069e-04,  ..., -1.0604e-02,\n",
      "          1.7755e-02, -1.6745e-02],\n",
      "        [-4.8059e-04,  5.7842e-03, -4.0765e-03,  ...,  7.5823e-03,\n",
      "         -6.8344e-03,  1.4575e-02],\n",
      "        [-2.4931e-03, -4.0972e-05, -3.0429e-03,  ...,  1.0234e-02,\n",
      "         -7.4270e-03,  1.3526e-02],\n",
      "        ...,\n",
      "        [-1.4040e-02,  3.8704e-02, -8.2814e-03,  ...,  1.5471e-03,\n",
      "         -2.7765e-04,  6.9827e-03],\n",
      "        [ 5.4474e-03, -1.6309e-02,  6.6378e-03,  ..., -4.0791e-03,\n",
      "         -4.5629e-04, -9.4568e-03],\n",
      "        [ 6.3939e-03, -1.5191e-03,  2.3360e-03,  ..., -9.9056e-03,\n",
      "          6.5391e-03, -1.1387e-02]]) tensor([[-6.7510e-04,  1.2098e-02, -3.9069e-04,  ..., -1.0604e-02,\n",
      "          1.7755e-02, -1.6745e-02],\n",
      "        [-4.8059e-04,  5.7842e-03, -4.0765e-03,  ...,  7.5823e-03,\n",
      "         -6.8344e-03,  1.4575e-02],\n",
      "        [-2.4931e-03, -4.1031e-05, -3.0429e-03,  ...,  1.0234e-02,\n",
      "         -7.4270e-03,  1.3526e-02],\n",
      "        ...,\n",
      "        [-1.4040e-02,  3.8704e-02, -8.2814e-03,  ...,  1.5471e-03,\n",
      "         -2.7765e-04,  6.9827e-03],\n",
      "        [ 5.4474e-03, -1.6309e-02,  6.6378e-03,  ..., -4.0791e-03,\n",
      "         -4.5628e-04, -9.4568e-03],\n",
      "        [ 6.3939e-03, -1.5191e-03,  2.3360e-03,  ..., -9.9056e-03,\n",
      "          6.5391e-03, -1.1387e-02]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(w2.grad, dw2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "shape mismatch: indexing tensors could not be broadcast together with shapes [32], [27]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m (\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m27\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m27\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m)\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[1;31mIndexError\u001b[0m: shape mismatch: indexing tensors could not be broadcast together with shapes [32], [27]"
     ]
    }
   ],
   "source": [
    "(torch.randn((32, 27))[range(n), torch.arange(27)]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-2., grad_fn=<SumBackward0>)\n",
      "1\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\brodi\\AppData\\Local\\Temp\\ipykernel_12988\\1971916642.py:8: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:494.)\n",
      "  print(A.grad)\n"
     ]
    }
   ],
   "source": [
    "A = torch.tensor([[1., 2.], [3., 4.]], requires_grad=True).max(1, keepdim=True).values\n",
    "B = torch.tensor([[1., 2.], [3., 4.]], requires_grad=True)\n",
    "f = (B-A).sum()\n",
    "print(f)\n",
    "expected = 1\n",
    "print(expected)\n",
    "f.backward()\n",
    "print(A.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.7980,  0.0166, -1.6127,  0.1601],\n",
      "        [ 1.1134,  0.1981, -2.3602,  0.2418],\n",
      "        [ 0.4975,  0.9954, -1.6254,  0.2032]])\n",
      "tensor([[ 0.7980,  0.0166, -1.6127,  0.1601],\n",
      "        [ 1.1134,  0.1981, -2.3602,  0.2418],\n",
      "        [ 0.4975,  0.9954, -1.6254,  0.2032]])\n"
     ]
    }
   ],
   "source": [
    "Y = torch.zeros((3, 4))\n",
    "X = torch.randn((3, 2))\n",
    "A = torch.randn((2, 4))\n",
    "for i in range(X.size(0)):\n",
    "    for j in range(A.size(1)):\n",
    "        Y[i, j] = sum(X[i, k] * A[k, j] for k in range(A.size(0)))\n",
    "print(Y)\n",
    "print(X @ A)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
